use "std.io";
use "std.string";
use "database"; // Imports database.prox

// --- FFI Declarations for libcurl ---
extern "libcurl.so" "curl_global_init" func curl_global_init(flags long) int;
extern "libcurl.so" "curl_easy_init" func curl_easy_init() pointer;
extern "libcurl.so" "curl_easy_setopt" func curl_easy_setopt(handle pointer, option int, parameter pointer) int;
extern "libcurl.so" "curl_easy_perform" func curl_easy_perform(handle pointer) int;
extern "libcurl.so" "curl_easy_cleanup" func curl_easy_cleanup(handle pointer) void;

// Constants
let CURLOPT_URL = 10002;
let CURLOPT_FOLLOWLOCATION = 52;
let CURLOPT_WRITEFUNCTION = 20011;

// Global buffer to store fetched HTML content
let global_html_buffer = "";

// Callback function for Curl to capture data into variable
func write_callback(data pointer, size int, nmemb int, user_data pointer) int {
    let total_size = size * nmemb;
    // Convert raw pointer data to string and append to global buffer
    let chunk = string(data, total_size); 
    global_html_buffer = global_html_buffer + chunk;
    return total_size;
}

func fetch_html(url string) string {
    global_html_buffer = ""; // Reset buffer
    
    curl_global_init(3);
    let curl = curl_easy_init();
    
    if (curl == null) {
        print("Failed to init curl");
        return "";
    }

    // Configure Curl
    curl_easy_setopt(curl, CURLOPT_URL, url);
    curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1);
    curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, write_callback); // Hook up the callback

    print("Fetching: " + url);
    let res = curl_easy_perform(curl);
    
    curl_easy_cleanup(curl);
    
    if (res != 0) {
        print("Error fetching URL code: " + res);
        return "";
    }
    
    return global_html_buffer;
}

// Smart Parsing Logic (Handles <title class="..."> etc.)
func extract_smart(html string, tag string) string {
    // 1. Find "<tag"
    let start_marker = "<" + tag;
    let start_index = html.index_of(start_marker);
    if (start_index == -1) { return ""; }
    
    // 2. Find the closing bracket ">" of the opening tag
    let tag_close_index = html.index_of(">", start_index);
    if (tag_close_index == -1) { return ""; }
    
    let content_start = tag_close_index + 1;
    
    // 3. Find closing tag "</tag>"
    let end_marker = "</" + tag + ">";
    let end_index = html.index_of(end_marker, content_start);
    if (end_index == -1) { return ""; }
    
    return html.substring(content_start, end_index);
}

func parse_and_save(url string, html string) void {
    let title = extract_smart(html, "title");
    let body = extract_smart(html, "body"); // Or "p", "div" based on need
    
    if (title == "") { title = "No Title"; }
    if (body == "") { body = "No Content Available"; }
    
    // Save using Database Module
    database.append_to_index(title, url, body);
}

func main() {
    // List of 10-20 Real Sites to Crawl
    let urls = [
        "https://www.wikipedia.org/",
        "https://news.ycombinator.com/",
        "https://www.python.org/",
        "https://reactnative.dev/",
        "https://github.com/explore",
        "https://stackoverflow.com/",
        "https://www.reddit.com/r/programming/",
        "https://dev.to/",
        "https://techcrunch.com/",
        "https://www.bbc.com/news/technology"
    ];
    
    let i = 0;
    while (i < urls.length()) {
        let url = urls[i];
        let html = fetch_html(url);
        
        if (html.length() > 100) { // Only save if we got substantial data
            parse_and_save(url, html);
        } else {
            print("Skipping empty response for: " + url);
        }
        
        i = i + 1;
    }
    print("Crawling Completed.");
}